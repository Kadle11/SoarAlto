diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 6264dd47a647..49c37152f95a 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -188,6 +188,7 @@ extern void mpol_put_task_policy(struct task_struct *);
 
 extern bool numa_demotion_enabled;
 extern bool numa_promotion_tiered_enabled;
+extern bool numa_skip_node_one;
 
 static inline bool mpol_is_preferred_many(struct mempolicy *pol)
 {
@@ -311,6 +312,7 @@ static inline void check_toptier_balanced(void)
 
 #define numa_demotion_enabled	false
 #define numa_promotion_tiered_enabled	false
+#define numa_skip_node_one	false
 
 static inline bool mpol_is_preferred_many(struct mempolicy *pol)
 {
diff --git a/include/linux/sched.h b/include/linux/sched.h
index c1a927ddec64..15f1176c0c27 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1282,6 +1282,9 @@ struct task_struct {
 	unsigned long			numa_faults_locality[3];
 
 	unsigned long			numa_pages_migrated;
+
+	unsigned long			numa_balancing_page_cnt;
+	unsigned long 			numa_balancing_page_promote_cnt;
 #endif /* CONFIG_NUMA_BALANCING */
 
 #ifdef CONFIG_RSEQ
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index bc6d2f44c01a..208e1a4cfd11 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -40,6 +40,10 @@ enum sched_tunable_scaling {
 #define NUMA_BALANCING_TIERED_MEMORY	0x2
 
 extern int sysctl_numa_balancing_mode;
+extern unsigned int sysctl_numa_balancing_disable_migrate;
+extern unsigned int sysctl_numa_balancing_page_promote_scale;
+
+extern int sysctl_numa_balancing_skip_node_one;
 
 /*
  *  control realtime throttling:
@@ -78,6 +82,8 @@ int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
 		void *buffer, size_t *lenp, loff_t *ppos);
 int sysctl_numa_balancing(struct ctl_table *table, int write, void *buffer,
 		size_t *lenp, loff_t *ppos);
+int sysctl_numa_balancing_skip_one(struct ctl_table *table, int write, void *buffer,
+		size_t *lenp, loff_t *ppos);
 int sysctl_schedstats(struct ctl_table *table, int write, void *buffer,
 		size_t *lenp, loff_t *ppos);
 
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 58fa7ec01637..18c51456beea 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4228,6 +4228,9 @@ DEFINE_STATIC_KEY_FALSE(sched_numa_balancing);
 int sysctl_numa_balancing_mode;
 bool numa_promotion_tiered_enabled;
 
+int sysctl_numa_balancing_skip_node_one;
+bool numa_skip_node_one;
+
 #ifdef CONFIG_NUMA_BALANCING
 
 /*
@@ -4279,6 +4282,21 @@ int sysctl_numa_balancing(struct ctl_table *table, int write,
 	}
 	return err;
 }
+
+int sysctl_numa_balancing_skip_one(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos)
+{
+	int err;
+	if (write && !capable(CAP_SYS_ADMIN))
+		return -EPERM;
+	err = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
+	if (err < 0)
+		return err;
+	if (write)
+		if (sysctl_numa_balancing_skip_node_one)
+			numa_skip_node_one = true;
+	return err;
+}
 #endif
 #endif
 
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index a9464e92c0ec..4640a7c852aa 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1071,6 +1071,11 @@ unsigned int sysctl_numa_balancing_scan_size = 256;
 /* Scan @scan_size MB every @scan_period after an initial @scan_delay in ms */
 unsigned int sysctl_numa_balancing_scan_delay = 1000;
 
+unsigned int sysctl_numa_balancing_disable_migrate = 0;
+
+unsigned int numa_balancing_page_cnt_limit = 10;
+unsigned int sysctl_numa_balancing_page_promote_scale = 10;
+
 struct numa_group {
 	refcount_t refcount;
 
@@ -1437,8 +1442,32 @@ bool should_numa_migrate_memory(struct task_struct *p, struct page * page,
 	 * The pages in non-toptier memory node should be migrated
 	 * according to hot/cold instead of accessing CPU node.
 	 */
-	if (numa_promotion_tiered_enabled && !node_is_toptier(src_nid))
-		return true;
+	if (numa_promotion_tiered_enabled && !node_is_toptier(src_nid)) {
+		if (sysctl_numa_balancing_disable_migrate) {
+			return false;
+		}
+		if (sysctl_numa_balancing_page_promote_scale > numa_balancing_page_cnt_limit) {
+			return true;
+		}
+		if (p->numa_balancing_page_promote_cnt < sysctl_numa_balancing_page_promote_scale) {
+			p->numa_balancing_page_cnt += 1;
+			p->numa_balancing_page_promote_cnt += 1;
+			if (p->numa_balancing_page_cnt == numa_balancing_page_cnt_limit) {
+				p->numa_balancing_page_cnt = 0;
+				p->numa_balancing_page_promote_cnt = 0;
+			}
+			return true;
+		}
+		if (p->numa_balancing_page_cnt < numa_balancing_page_cnt_limit) {
+			p->numa_balancing_page_cnt += 1;
+			if (p->numa_balancing_page_cnt == numa_balancing_page_cnt_limit) {
+				p->numa_balancing_page_cnt = 0;
+				p->numa_balancing_page_promote_cnt = 0;
+			}
+			return false;
+		}
+		return false;
+	}
 
 	/*
 	 * Allow first faults or private faults to migrate immediately early in
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 0f3e709a2a48..20f384ae4059 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -1812,6 +1812,31 @@ static struct ctl_table kern_table[] = {
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= &three,
 	},
+	{
+		.procname	= "numa_balancing_disable_migrate",
+		.data		= &sysctl_numa_balancing_disable_migrate,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+	},
+	{
+		.procname	= "numa_balancing_page_promote_scale",
+		.data		= &sysctl_numa_balancing_page_promote_scale,
+		.maxlen         = sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+	},
+	{
+		.procname	= "numa_balancing_skip_node_one",
+		.data		= &sysctl_numa_balancing_skip_node_one,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= sysctl_numa_balancing_skip_one,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
 #endif /* CONFIG_NUMA_BALANCING */
 	{
 		.procname	= "sched_rt_period_us",
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 8921cb5603a6..c429c2a24b79 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -2172,6 +2172,10 @@ struct page *alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,
 
 	nmask = policy_nodemask(gfp, pol);
 	preferred_nid = policy_node(gfp, pol, node);
+
+	if (numa_skip_node_one && preferred_nid == 1)
+		preferred_nid=2;
+
 	page = __alloc_pages(gfp, order, preferred_nid, nmask);
 	mpol_cond_put(pol);
 out:
diff --git a/mm/migrate.c b/mm/migrate.c
index 12e0a63181a4..dddbbe56cd65 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -1184,6 +1184,9 @@ int next_demotion_node(int node)
 	target = READ_ONCE(node_demotion[node]);
 	rcu_read_unlock();
 
+	if (numa_skip_node_one && target == 1)
+		return 2;
+
 	return target;
 }
 
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 9fb1cfc068bc..d2d4cad16726 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -114,6 +114,9 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 				if (target_node == nid)
 					continue;
 
+				if (numa_skip_node_one && nid ==1)
+					continue;
+
 				/* skip scanning toptier node */
 				if (numa_promotion_tiered_enabled && node_is_toptier(nid))
 					continue;
