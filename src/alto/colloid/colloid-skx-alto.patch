diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index d232de7cdc56..44aa6ab18b8e 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -174,7 +174,7 @@ extern void mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol);
 /* Check if a vma is migratable */
 extern bool vma_migratable(struct vm_area_struct *vma);
 
-extern int mpol_misplaced(struct page *, struct vm_area_struct *, unsigned long);
+extern int mpol_misplaced(struct page *, struct vm_area_struct *, unsigned long, int *timestamps);
 extern void mpol_put_task_policy(struct task_struct *);
 
 static inline bool mpol_is_preferred_many(struct mempolicy *pol)
@@ -279,7 +279,7 @@ static inline int mpol_parse_str(char *str, struct mempolicy **mpol)
 #endif
 
 static inline int mpol_misplaced(struct page *page, struct vm_area_struct *vma,
-				 unsigned long address)
+				 unsigned long address, int *timestamps)
 {
 	return -1; /* no node preference */
 }
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 63d242164b1a..9242ae389ecf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1298,6 +1298,9 @@ struct task_struct {
 	unsigned long			numa_faults_locality[3];
 
 	unsigned long			numa_pages_migrated;
+
+	unsigned long			numa_balancing_page_cnt;
+	unsigned long			numa_balancing_page_promote_cnt;
 #endif /* CONFIG_NUMA_BALANCING */
 
 #ifdef CONFIG_RSEQ
diff --git a/include/linux/sched/numa_balancing.h b/include/linux/sched/numa_balancing.h
index ae085ae29d90..b89ad65f075b 100644
--- a/include/linux/sched/numa_balancing.h
+++ b/include/linux/sched/numa_balancing.h
@@ -21,8 +21,8 @@ extern pid_t task_numa_group_id(struct task_struct *p);
 extern void set_numabalancing_state(bool enabled);
 extern void task_numa_free(struct task_struct *p, bool final);
 extern bool should_numa_migrate_memory(struct task_struct *p, struct page *page,
-					int src_nid, int dst_cpu);
-extern int numa_migrate_memory_away_target(struct page *page, int src_nid);
+					int src_nid, int dst_cpu, int *timestamps);
+extern int numa_migrate_memory_away_target(struct page *page, int src_nid, int *timestamps);
 #else
 static inline void task_numa_fault(int last_node, int node, int pages,
 				   int flags)
@@ -39,11 +39,11 @@ static inline void task_numa_free(struct task_struct *p, bool final)
 {
 }
 static inline bool should_numa_migrate_memory(struct task_struct *p,
-				struct page *page, int src_nid, int dst_cpu)
+				struct page *page, int src_nid, int dst_cpu, int *timestamps)
 {
 	return true;
 }
-static inline int numa_migrate_memory_away_target(struct page *page, int src_nid)
+static inline int numa_migrate_memory_away_target(struct page *page, int src_nid, int *timestamps)
 {
 	return NUMA_NO_NODE;
 }
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 8abd19833a98..8bec358ca014 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -26,6 +26,11 @@ enum sched_tunable_scaling {
 
 #ifdef CONFIG_NUMA_BALANCING
 extern int sysctl_numa_balancing_mode;
+extern unsigned int sysctl_numa_balancing_disable_migrate;
+extern unsigned int sysctl_numa_balancing_page_promote_scale;
+extern unsigned int sysctl_numa_balancing_reset_kswapd_failures;
+extern unsigned int sysctl_numa_balancing_disable_pte;
+extern unsigned int sysctl_numa_balancing_pte_scale;
 #else
 #define sysctl_numa_balancing_mode	0
 #endif
diff --git a/include/trace/events/kmem.h b/include/trace/events/kmem.h
index 58688768ef0f..dde3ff57caa1 100644
--- a/include/trace/events/kmem.h
+++ b/include/trace/events/kmem.h
@@ -371,6 +371,36 @@ TRACE_EVENT(rss_stat,
 		__print_symbolic(__entry->member, TRACE_MM_PAGES),
 		__entry->size)
 	);
+
+TRACE_EVENT(do_numa_page,
+
+	TP_PROTO(int pid, unsigned long address, int unmap_time, int access_time, bool success),
+
+	TP_ARGS(pid, address, unmap_time, access_time, success),
+
+	TP_STRUCT__entry(
+		__field(int, pid)
+		__field(unsigned long, address)
+		__field(int, unmap_time)
+		__field(int, access_time)
+		__field(bool, success)
+	),
+
+	TP_fast_assign(
+		__entry->pid = pid;
+		__entry->address = address;
+		__entry->unmap_time = unmap_time;
+		__entry->access_time = access_time;
+		__entry->success = success;
+	),
+
+	TP_printk("pid=%d address=0x%lx unmap_time=%d access_time=%d success=%d",
+		__entry->pid,
+		__entry->address,
+		__entry->unmap_time,
+		__entry->access_time,
+		__entry->success)
+);
 #endif /* _TRACE_KMEM_H */
 
 /* This part must be outside protection */
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index d1c4e8a98103..c8b1cefab6d8 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4642,6 +4642,46 @@ static struct ctl_table sched_core_sysctls[] = {
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= SYSCTL_ONE_HUNDRED,
 	},
+	{
+		.procname	= "numa_balancing_disable_migrate",
+		.data		= &sysctl_numa_balancing_disable_migrate,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+	},
+	{
+		.procname	= "numa_balancing_page_promote_scale",
+		.data		= &sysctl_numa_balancing_page_promote_scale,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+	},
+	{
+		.procname	= "numa_balancing_reset_kswapd_failures",
+		.data		= &sysctl_numa_balancing_reset_kswapd_failures,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+	},
+	{
+		.procname	= "numa_balancing_disable_pte",
+		.data		= &sysctl_numa_balancing_disable_pte,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+	},
+	{
+		.procname	= "numa_balancing_pte_scale",
+		.data		= &sysctl_numa_balancing_pte_scale,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+	},
 #endif /* CONFIG_NUMA_BALANCING */
 	{}
 };
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index bab83e570440..f8183d99b9fc 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1109,6 +1109,14 @@ unsigned int sysctl_numa_balancing_scan_size = 256;
 /* Scan @scan_size MB every @scan_period after an initial @scan_delay in ms */
 unsigned int sysctl_numa_balancing_scan_delay = 1000;
 
+unsigned int sysctl_numa_balancing_disable_migrate = 0;
+
+unsigned int numa_balancing_page_cnt_limit = 10;
+unsigned int sysctl_numa_balancing_page_promote_scale = 10;
+
+unsigned int sysctl_numa_balancing_reset_kswapd_failures = 0;
+unsigned int sysctl_numa_balancing_disable_pte = 0;
+
 /* The page with hint page fault latency < threshold in ms is considered hot */
 unsigned int sysctl_numa_balancing_hot_threshold = MSEC_PER_SEC;
 
@@ -1506,13 +1514,16 @@ static bool pgdat_free_space_enough(struct pglist_data *pgdat)
  * The smaller the hint page fault latency, the higher the possibility
  * for the page to be hot.
  */
-static int numa_hint_fault_latency(struct page *page)
+static int numa_hint_fault_latency(struct page *page, int *timestamps)
 {
 	int last_time, time;
 
 	time = jiffies_to_msecs(jiffies);
 	last_time = xchg_page_access_time(page, time);
 
+	timestamps[0] = last_time & PAGE_ACCESS_TIME_MASK;
+	timestamps[1] = time & PAGE_ACCESS_TIME_MASK;
+
 	return (time - last_time) & PAGE_ACCESS_TIME_MASK;
 }
 
@@ -1569,7 +1580,7 @@ static void numa_promotion_adjust_threshold(struct pglist_data *pgdat,
 }
 
 bool should_numa_migrate_memory(struct task_struct *p, struct page * page,
-				int src_nid, int dst_cpu)
+				int src_nid, int dst_cpu, int *timestamps)
 {
 	struct numa_group *ng = deref_curr_numa_group(p);
 	int dst_nid = cpu_to_node(dst_cpu);
@@ -1609,7 +1620,7 @@ bool should_numa_migrate_memory(struct task_struct *p, struct page * page,
 		numa_promotion_adjust_threshold(pgdat, rate_limit, def_th);
 
 		th = pgdat->nbp_threshold ? : def_th;
-		latency = numa_hint_fault_latency(page);
+		latency = numa_hint_fault_latency(page, timestamps);
 		if (latency >= th)
 			return false;
 
@@ -1630,6 +1641,33 @@ bool should_numa_migrate_memory(struct task_struct *p, struct page * page,
 	this_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);
 	last_cpupid = page_cpupid_xchg_last(page, this_cpupid);
 
+	if (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) {
+		if (sysctl_numa_balancing_disable_migrate) {
+			return false;
+		}
+		if (sysctl_numa_balancing_page_promote_scale > numa_balancing_page_cnt_limit) {
+			return true;
+		}
+		if (p->numa_balancing_page_promote_cnt < sysctl_numa_balancing_page_promote_scale) {
+			p->numa_balancing_page_cnt += 1;
+			p->numa_balancing_page_promote_cnt += 1;
+			if (p->numa_balancing_page_cnt == numa_balancing_page_cnt_limit) {
+				p->numa_balancing_page_cnt = 0;
+				p->numa_balancing_page_promote_cnt = 0;
+			}
+			return true;
+		}
+		if (p->numa_balancing_page_cnt < numa_balancing_page_cnt_limit) {
+			p->numa_balancing_page_cnt += 1;
+			if (p->numa_balancing_page_cnt == numa_balancing_page_cnt_limit) {
+				p->numa_balancing_page_cnt = 0;
+				p->numa_balancing_page_promote_cnt = 0;
+			}
+			return false;
+		}
+		return false;
+	}
+
 	if (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&
 	    !node_is_toptier(src_nid) && !cpupid_valid(last_cpupid))
 		return false;
@@ -1699,7 +1737,7 @@ Should we migrate page away from local NUMA node due to congestion?
 if yes, returns nid of destination node
 otherwise returns NUMA_NO_NODE
 */
-int numa_migrate_memory_away_target(struct page *page, int src_nid) {
+int numa_migrate_memory_away_target(struct page *page, int src_nid, int *timestamps) {
 	unsigned int latency, th;
 	// TODO: add condition to ignore mlocked / unevictable pages
 	if(!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING &&
@@ -1724,7 +1762,7 @@ int numa_migrate_memory_away_target(struct page *page, int src_nid) {
 	} else {
 		// Timestamp in page flags. use hint fault latency to determine hotness
 		th = NODE_DATA(src_nid)->nbp_threshold ? : sysctl_numa_balancing_hot_threshold;
-		latency = numa_hint_fault_latency(page);
+		latency = numa_hint_fault_latency(page, timestamps);
 		if (latency >= th)
 			return NUMA_NO_NODE;
 	}
@@ -2996,6 +3034,31 @@ static void reset_ptenuma_scan(struct task_struct *p)
 	p->mm->numa_scan_offset = 0;
 }
 
+unsigned int sysctl_numa_balancing_pte_scale = 16;
+unsigned int numa_balancing_pte_scale_max = 16;
+static unsigned long change_prot_numa_scale(struct vm_area_struct *vma,
+		                        unsigned long addr, unsigned long end, unsigned long *end_p)
+{
+	int nr_updated;
+	unsigned long end_n;
+	unsigned long size_n;
+	if (sysctl_numa_balancing_pte_scale >= numa_balancing_pte_scale_max) {
+		end_n = end;
+	} else {
+		if (end > addr) {
+			size_n = end - addr;
+			end_n = addr + (size_n / numa_balancing_pte_scale_max) * sysctl_numa_balancing_pte_scale;
+			end_n = ALIGN(end_n, HPAGE_SIZE);
+			*end_p = end_n;
+			if (end_n - addr < HPAGE_SIZE) {
+				return nr_updated;
+			}
+		}
+	}
+	nr_updated = change_prot_numa(vma, addr, end_n);
+	return nr_updated;
+}
+
 /*
  * The expensive part of numa migration is done from task_work context.
  * Triggered from task_tick_numa().
@@ -3008,6 +3071,7 @@ static void task_numa_work(struct callback_head *work)
 	u64 runtime = p->se.sum_exec_runtime;
 	struct vm_area_struct *vma;
 	unsigned long start, end;
+	unsigned long end_n;
 	unsigned long nr_pte_updates = 0;
 	long pages, virtpages;
 	struct vma_iterator vmi;
@@ -3099,7 +3163,10 @@ static void task_numa_work(struct callback_head *work)
 			start = max(start, vma->vm_start);
 			end = ALIGN(start + (pages << PAGE_SHIFT), HPAGE_SIZE);
 			end = min(end, vma->vm_end);
-			nr_pte_updates = change_prot_numa(vma, start, end);
+			end_n = end;
+			if (sysctl_numa_balancing_disable_pte == 0) {
+				nr_pte_updates = change_prot_numa_scale(vma, start, end, &end_n);
+			}
 
 			/*
 			 * Try to scan sysctl_numa_balancing_size worth of
@@ -3110,8 +3177,8 @@ static void task_numa_work(struct callback_head *work)
 			 * areas faster.
 			 */
 			if (nr_pte_updates)
-				pages -= (end - start) >> PAGE_SHIFT;
-			virtpages -= (end - start) >> PAGE_SHIFT;
+				pages -= (end_n - start) >> PAGE_SHIFT;
+			virtpages -= (end_n - start) >> PAGE_SHIFT;
 
 			start = end;
 			if (pages <= 0 || virtpages <= 0)
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 7e21fa58f31e..9d41a8e285bc 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1510,6 +1510,7 @@ vm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf)
 	int target_nid, last_cpupid = (-1 & LAST_CPUPID_MASK);
 	bool migrated = false, writable = false;
 	int flags = 0;
+	int timestamps[2] = {0, 0};
 
 	vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
 	if (unlikely(!pmd_same(oldpmd, *vmf->pmd))) {
@@ -1544,14 +1545,14 @@ vm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf)
 	if (node_is_toptier(page_nid))
 		last_cpupid = page_cpupid_last(page);
 	target_nid = numa_migrate_prep(page, vma, haddr, page_nid,
-				       &flags);
+				       &flags, timestamps);
 
 	/*
 	colloid
 	Move pages away from local NUMA is congested
 	*/
 	if(page_nid == numa_node_id() && target_nid == NUMA_NO_NODE) {
-		target_nid = numa_migrate_memory_away_target(page, page_nid);
+		target_nid = numa_migrate_memory_away_target(page, page_nid, timestamps);
 	}
 
 	if (target_nid == NUMA_NO_NODE) {
diff --git a/mm/internal.h b/mm/internal.h
index 7920a8b7982e..7522e6c385d3 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -853,7 +853,7 @@ void vunmap_range_noflush(unsigned long start, unsigned long end);
 void __vunmap_range_noflush(unsigned long start, unsigned long end);
 
 int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
-		      unsigned long addr, int page_nid, int *flags);
+		      unsigned long addr, int page_nid, int *flags, int *timestamps);
 
 void free_zone_device_page(struct page *page);
 int migrate_device_coherent_page(struct page *page);
diff --git a/mm/memory.c b/mm/memory.c
index 1ba11cb216b8..8b404c2d3e4d 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -4647,7 +4647,7 @@ static vm_fault_t do_fault(struct vm_fault *vmf)
 }
 
 int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
-		      unsigned long addr, int page_nid, int *flags)
+		      unsigned long addr, int page_nid, int *flags, int *timestamps)
 {
 	get_page(page);
 
@@ -4657,7 +4657,7 @@ int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
 		*flags |= TNF_FAULT_LOCAL;
 	}
 
-	return mpol_misplaced(page, vma, addr);
+	return mpol_misplaced(page, vma, addr, timestamps);
 }
 
 static vm_fault_t do_numa_page(struct vm_fault *vmf)
@@ -4670,6 +4670,9 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)
 	int target_nid;
 	pte_t pte, old_pte;
 	int flags = 0;
+	int timestamps[2] = {0, 0};
+	int pid = 0;
+	int success = 1;
 
 	/*
 	 * The "pte" at this point cannot be used safely without
@@ -4732,14 +4735,16 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)
 		last_cpupid = (-1 & LAST_CPUPID_MASK);
 	else
 		last_cpupid = page_cpupid_last(page);
+	// latency checked in here
 	target_nid = numa_migrate_prep(page, vma, vmf->address, page_nid,
-			&flags);
+			&flags, timestamps);
 	/*
 	colloid
 	Move pages away from local NUMA is congested
 	*/
 	if(page_nid == numa_node_id() && target_nid == NUMA_NO_NODE) {
-		target_nid = numa_migrate_memory_away_target(page, page_nid);
+		target_nid = numa_migrate_memory_away_target(page, page_nid, timestamps);
+		success = 2; /* success = 2 means colloid's demotion in effect */
 	}
 	if (target_nid == NUMA_NO_NODE) {
 		put_page(page);
@@ -4748,11 +4753,15 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
 	writable = false;
 
+	pid = vma->vm_mm->owner->pid;
+
 	/* Migrate to the requested node */
 	if (migrate_misplaced_page(page, vma, target_nid)) {
+		trace_do_numa_page(pid, vmf->real_address, timestamps[0], timestamps[1], success);
 		page_nid = target_nid;
 		flags |= TNF_MIGRATED;
 	} else {
+		trace_do_numa_page(pid, vmf->real_address, timestamps[0], timestamps[1], 0);
 		flags |= TNF_MIGRATE_FAIL;
 		vmf->pte = pte_offset_map(vmf->pmd, vmf->address);
 		spin_lock(vmf->ptl);
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 2068b594dc88..63e6aef5ff44 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -2556,7 +2556,7 @@ static void sp_free(struct sp_node *n)
  * Return: NUMA_NO_NODE if the page is in a node that is valid for this
  * policy, or a suitable node ID to allocate a replacement page from.
  */
-int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long addr)
+int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long addr, int *timestamps)
 {
 	struct mempolicy *pol;
 	struct zoneref *z;
@@ -2620,7 +2620,7 @@ int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long
 	if (pol->flags & MPOL_F_MORON) {
 		polnid = thisnid;
 
-		if (!should_numa_migrate_memory(current, page, curnid, thiscpu))
+		if (!should_numa_migrate_memory(current, page, curnid, thiscpu, timestamps))
 			goto out;
 	}
 
diff --git a/mm/migrate.c b/mm/migrate.c
index 52da37e0e6e5..ec24efb6f002 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -2553,6 +2553,11 @@ static int numamigrate_isolate_page(pg_data_t *pgdat, struct page *page)
 			if (managed_zone(pgdat->node_zones + z))
 				break;
 		}
+
+		if (sysctl_numa_balancing_reset_kswapd_failures) {
+			pgdat->kswapd_failures = 0;
+		}
+
 		wakeup_kswapd(pgdat->node_zones + z, 0, order, ZONE_MOVABLE);
 		return 0;
 	}
