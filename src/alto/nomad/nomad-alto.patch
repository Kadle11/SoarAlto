diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 0a76ac103b17..c1ea7874aeee 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -190,6 +190,7 @@ extern void mpol_put_task_policy(struct task_struct *);
 
 extern bool numa_demotion_enabled;
 extern bool numa_promotion_tiered_enabled;
+extern bool numa_skip_node_one;
 
 #else
 
@@ -306,5 +307,6 @@ static inline void check_toptier_balanced(void)
 
 #define numa_demotion_enabled	false
 #define numa_promotion_tiered_enabled	false
+#define numa_skip_node_one	false
 #endif /* CONFIG_NUMA */
 #endif
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 8247c55cf0d7..c3e782a790f7 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -45,6 +45,10 @@ extern unsigned int sysctl_sched_tunable_scaling;
 #define NUMA_BALANCING_TIERED_MEMORY	0x2
 
 extern int sysctl_numa_balancing_mode;
+extern unsigned int sysctl_numa_balancing_disable_pte;
+extern unsigned int sysctl_numa_balancing_pte_scale;
+
+extern int sysctl_numa_balancing_skip_node_one;
 
 extern unsigned int sysctl_numa_balancing_scan_delay;
 extern unsigned int sysctl_numa_balancing_scan_period_min;
@@ -96,6 +100,8 @@ int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
 		void *buffer, size_t *lenp, loff_t *ppos);
 int sysctl_numa_balancing(struct ctl_table *table, int write, void *buffer,
 		size_t *lenp, loff_t *ppos);
+int sysctl_numa_balancing_skip_one(struct ctl_table *table, int write, void *buffer,
+		size_t *lenp, loff_t *ppos);
 int sysctl_schedstats(struct ctl_table *table, int write, void *buffer,
 		size_t *lenp, loff_t *ppos);
 
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 733e5464fd71..cd65ba1dd712 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3614,6 +3614,9 @@ DEFINE_STATIC_KEY_FALSE(sched_numa_balancing);
 int sysctl_numa_balancing_mode;
 bool numa_promotion_tiered_enabled;
 
+int sysctl_numa_balancing_skip_node_one;
+bool numa_skip_node_one;
+
 #ifdef CONFIG_NUMA_BALANCING
 
 /*
@@ -3664,6 +3667,21 @@ int sysctl_numa_balancing(struct ctl_table *table, int write,
 	}
 	return err;
 }
+
+int sysctl_numa_balancing_skip_one(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos)
+{
+	int err;
+	if (write && !capable(CAP_SYS_ADMIN))
+		return -EPERM;
+	err = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
+	if (err < 0)
+		return err;
+	if (write)
+		if (sysctl_numa_balancing_skip_node_one)
+			numa_skip_node_one = true;
+	return err;
+}
 #endif
 #endif
 
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index c33dd2652305..5dc19d71d203 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1082,6 +1082,10 @@ unsigned int sysctl_numa_balancing_scan_size = 256;
 /* Scan @scan_size MB every @scan_period after an initial @scan_delay in ms */
 unsigned int sysctl_numa_balancing_scan_delay = 1000;
 
+unsigned int sysctl_numa_balancing_disable_pte = 0;
+unsigned int sysctl_numa_balancing_pte_scale = 16;
+unsigned int numa_balancing_pte_scale_max = 16;
+
 struct numa_group {
 	refcount_t refcount;
 
@@ -2737,6 +2741,29 @@ static void reset_ptenuma_scan(struct task_struct *p)
 	p->mm->numa_scan_offset = 0;
 }
 
+static unsigned long change_prot_numa_scale(struct vm_area_struct *vma,
+		unsigned long addr, unsigned long end, unsigned long *end_p)
+{
+	int nr_updated;
+	unsigned long end_n;
+	unsigned long size_n;
+	if (sysctl_numa_balancing_pte_scale >= numa_balancing_pte_scale_max) {
+		end_n = end;
+	} else {
+		if (end > addr) {
+			size_n = end - addr;
+			end_n = addr + (size_n / numa_balancing_pte_scale_max) * sysctl_numa_balancing_pte_scale;
+			end_n = ALIGN(end_n, HPAGE_SIZE);
+			*end_p = end_n;
+			if (end_n - addr < HPAGE_SIZE) {
+				return nr_updated;
+			}
+		}
+	}
+	nr_updated = change_prot_numa(vma, addr, end_n);
+	return nr_updated;
+}
+
 /*
  * The expensive part of numa migration is done from task_work context.
  * Triggered from task_tick_numa().
@@ -2749,6 +2776,7 @@ static void task_numa_work(struct callback_head *work)
 	u64 runtime = p->se.sum_exec_runtime;
 	struct vm_area_struct *vma;
 	unsigned long start, end;
+	unsigned long end_n;
 	unsigned long nr_pte_updates = 0;
 	long pages, virtpages;
 
@@ -2836,7 +2864,11 @@ static void task_numa_work(struct callback_head *work)
 			start = max(start, vma->vm_start);
 			end = ALIGN(start + (pages << PAGE_SHIFT), HPAGE_SIZE);
 			end = min(end, vma->vm_end);
-			nr_pte_updates = change_prot_numa(vma, start, end);
+			end_n = end;
+			if (sysctl_numa_balancing_disable_pte == 0) {
+				//nr_pte_updates = change_prot_numa(vma, start, end);
+				nr_pte_updates = change_prot_numa_scale(vma, start, end, &end_n);
+			}
 
 			/*
 			 * Try to scan sysctl_numa_balancing_size worth of
@@ -2847,8 +2879,8 @@ static void task_numa_work(struct callback_head *work)
 			 * areas faster.
 			 */
 			if (nr_pte_updates)
-				pages -= (end - start) >> PAGE_SHIFT;
-			virtpages -= (end - start) >> PAGE_SHIFT;
+				pages -= (end_n - start) >> PAGE_SHIFT;
+			virtpages -= (end_n - start) >> PAGE_SHIFT;
 
 			start = end;
 			if (pages <= 0 || virtpages <= 0)
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 4f236326b562..96205c57c0f8 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -1759,6 +1759,31 @@ static struct ctl_table kern_table[] = {
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= &three,
 	},
+	{
+		.procname	= "numa_balancing_disable_pte",
+		.data		= &sysctl_numa_balancing_disable_pte,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+	},
+	{
+		.procname	= "numa_balancing_pte_scale",
+		.data		= &sysctl_numa_balancing_pte_scale,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+	},
+	{
+		.procname	= "numa_balancing_skip_node_one",
+		.data		= &sysctl_numa_balancing_skip_node_one,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= sysctl_numa_balancing_skip_one,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
 #endif /* CONFIG_NUMA_BALANCING */
 	{
 		.procname	= "sched_rt_period_us",
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 7222ce4ea684..a554e0a55c72 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -2260,6 +2260,10 @@ struct page *alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,
 
 	nmask = policy_nodemask(gfp, pol);
 	preferred_nid = policy_node(gfp, pol, node);
+
+	if (numa_skip_node_one && preferred_nid == 1)
+		preferred_nid=2;
+
 	page = __alloc_pages(gfp, order, preferred_nid, nmask);
 	mpol_cond_put(pol);
 out:
diff --git a/mm/migrate.c b/mm/migrate.c
index ee780f39ba3d..961df1eb0316 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -1331,6 +1331,9 @@ int next_demotion_node(int node)
 	target = READ_ONCE(node_demotion[node]);
 	rcu_read_unlock();
 
+	if (numa_skip_node_one && target == 1)
+		return 2;
+
 	return target;
 }
 
diff --git a/mm/mprotect.c b/mm/mprotect.c
index e397a05c1d4b..40c0a7ff9792 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -114,6 +114,9 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 				if (target_node == nid)
 					continue;
 
+				if (numa_skip_node_one && nid ==1)
+					continue;
+
 				/* skip scanning toptier node */
 				if (numa_promotion_tiered_enabled && node_is_toptier(nid))
 					continue;
